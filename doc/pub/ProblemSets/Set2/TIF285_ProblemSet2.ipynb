{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2\n",
    "## Learning from data [TIF285], Chalmers, Fall 2019\n",
    "\n",
    "Last revised: 18-Sep-2019 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This problem set is performed individually (but collaboration is encouraged) and contains a number of basic and extra problems; you can choose which and how many to work on.\n",
    "- See examination rules on the course web page.\n",
    "- Hand-in is performed through the following **two** actions:\n",
    "  - Upload of your solution in the form of a jupyter notebook, or python code, via Canvas.\n",
    "  - Answer the corresponding questions on OpenTA.\n",
    "  \n",
    "  Note that the hand-in is not complete, and will not be graded, if any of those actions is not performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your personal details\n",
    "- Name: **Lastname, Firstname**\n",
    "- Personnummer: **YYMMDD-XXXX**\n",
    "  <br/>\n",
    "  (civic registration number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Coin tossing\n",
    "### (2 basic points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data with simulated coin tosses from the file `cointosses.dat`.\n",
    "Each row corresponds to a single toss: 0=tails; 1=heads\n",
    "\n",
    "Extract the mean and 95% DOB intervals from the first 8 tosses, the first 64 tosses, the first 512 tosses and all 4096 tosses in the data assuming a uniform prior for the probability $p_H$ of obtaining heads in a single toss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: Sample code for computing the DOB interval is available in the demonstration notebook `demo-BayesianBasics.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Straight line fitting\n",
    "### (2 basic points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the file `straightline.dat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're fitting a straight line to this data. Our model has two parameters $\\theta=[m,b]$\n",
    "\n",
    "$$\n",
    "y_M(x) = mx + b\n",
    "$$\n",
    "\n",
    "And our statistical model assumes that errors are normally distributed\n",
    "\n",
    "$$\n",
    "y_i = y_M(x_i;\\theta) + \\mathcal{N}(0, \\sigma),\n",
    "$$\n",
    "\n",
    "with fixed $\\sigma = 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume two different priors: a (log) flat prior and a (log) symmetric / scale-invariant prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_flat_prior(theta):\n",
    "    if np.all(np.abs(theta) < 1000):\n",
    "        return 0 # log(1)\n",
    "    else:\n",
    "        return -np.inf  # log(0)\n",
    "    \n",
    "def log_symmetric_prior(theta):\n",
    "    if np.abs(theta[0]) < 1000:\n",
    "        return -1.5 * np.log(1 + theta[1] ** 2)\n",
    "    else:\n",
    "        return -np.inf  # log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Where is the mode of the posterior with these two different priors?\n",
    "* Plot the joint pdf for the slope and the intercept for the two different prior choices.\n",
    "* Are the two parameters correlated or anticorrelated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Signal and background\n",
    "### (3 basic points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to estimate the amplitude of a signal when there is a background.  We'll take a limiting case where the background is flat, so it is completely specified by its magnitude $B > 0$, and the signal is known to be a Gaussian with unknown amplitude $A$ but (at least initially) known position (mean) and width (standard deviation). The measurements will be integer numbers of counts $\\{N_k\\}$ in well-defined (equally spaced) bins $\\{x_k\\}$, where $k$ runs over integers labeling the bins.  The goal therefore translates into finding $A$ and $B$ given $\\{N_k\\}$ and all the other information (bin sizes, signal position and width). After we can modify our goal if we do not care about $B$, or we care only about $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our statistical model includes the true signal plus a constant background. The signal and the background magnitudes are the unknown parameters while the other parameters dictating the signal (width $w$ and mean $x_0$ of the Gaussian) are known and fixed:\n",
    "\n",
    "$$\n",
    "   D_k = n_0 \\left[ A e^{-(x_k-x_0)^2/2 w^2} + B \\right]\n",
    "$$\n",
    "\n",
    "Here $n_0$ is a constant that scales with measurement time.  Note that $D_k$ is not an integer in general, unlike $N_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data according to the statistical model\n",
    "A_true = 1.\n",
    "B_true = 2.\n",
    "width = np.sqrt(5.)   \n",
    "x_0 = 0\n",
    "\n",
    "def exact_data(A, B, n_0, x_k, x_0=0., width=np.sqrt(5.)):\n",
    "    \"\"\"\n",
    "    Return the exact signal plus background.  The overall scale is n_0,\n",
    "    which is determined by how long counts are collected. \n",
    "    \"\"\"\n",
    "    return n_0 * (A * np.exp(-(x_k - x_0)**2/(2.*width**2)) + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson distribution\n",
    "We are imagining a counting experiment, so the statistics of the counts we record will follow a Poisson distribution.\n",
    "\n",
    "The Poisson discrete random variable from scipy.stats is defined by (see [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html))\n",
    "\n",
    "$$\n",
    "p(k \\mid \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!} \\quad \\mbox{for }k\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $k$ is an integer and $\\mu$ is called the shape parameter. The mean and variance of this distribution are both equal to $\\mu$. Sivia and Gregory each use a different notation for for this distribution, which means you should be flexible. \n",
    "\n",
    "For convenience, we'll define our own version in this notebook:\n",
    "\n",
    "$$\n",
    "p(N \\mid D) = \\frac{D^N e^{-D}}{N!} \\quad \\mbox{for }N\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $N$ is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset for exploring\n",
    "def make_dataset(A_true, B_true, width, x_0, databins, delta_x=1, D_max=100):\n",
    "    \"\"\"\n",
    "    Create a data set based on the number of bins (databins), the spacing\n",
    "    of bins (delta_x), and the maximum we want the exact result to have\n",
    "    (D_max, this fixes the n_0 parameter).\n",
    "    \n",
    "    Return arrays for the x points (xk_pts), the corresponding values of the\n",
    "    exact signal plus background in those bins (Dk_pts), the measured values\n",
    "    in those bins (Nk_pts, integers drawn from a Poisson distribution), the \n",
    "    maximum extent of the bins (x_max) and n_0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up evenly spaced bins, centered on x_0\n",
    "    x_max = x_0 + delta_x * (databins-1)/2\n",
    "    xk_pts = np.arange(-x_max, x_max + delta_x, delta_x, dtype=int)\n",
    "    \n",
    "    # scale n_0 so maximum of the \"true\" signal plus background is D_max\n",
    "    n_0 = D_max / (A_true + B_true)  \n",
    "    Dk_pts = exact_data(A_true, B_true, n_0, xk_pts, width=width)\n",
    "    \n",
    "    # sample for each k to determine the measured N_k\n",
    "    Nk_pts = [stats.poisson.rvs(mu=Dk) for Dk in Dk_pts]\n",
    "    \n",
    "    return xk_pts, Dk_pts, Nk_pts, x_max, n_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "* Make a plot of the true signal plus background we are trying to deduce. Use $A_\\mathrm{true}=1$ and $B_\\mathrm{true}=2$ and the signal position (mean) $x_0=0$ and width (standard deviation)  $w=\\sqrt{5}$.\n",
    "\n",
    "We consider what happens for fixed signal and background but changing the experimental conditions specified by `D_max` and `databins` (we'll keep `delta_x` fixed to 1). In all cases the bins are symmetric around $x=0$.\n",
    "* Make four subplots that correspond to the data from four differently designed counting experiments. \n",
    "  1. Baseline case: 15 bins and maximum expection of 100 counts.\n",
    "  1. Low statistics case: 15 bins and maximum expection of only 10 counts.\n",
    "  1. Greater range case: 31 bins (with fixed bin width) and same maximum expection of 100 counts as in baseline case.\n",
    "  1. Smaller range case: 7 bins (with fixed bin width) and same maximum expection of 100 counts as in baseline case.\n",
    "  \n",
    "* Implement functions for the (log) likelihood and for a uniform (log) prior. Let's use a uniform prior for $0 \\le A \\le 5$ and $0 \\le B \\le 5$.\n",
    "\n",
    "* You can then either evaluate the log-posterior on a grid, or use a MCMC sampler. Plot both the joint and the marginalized posterior pdfs for the four cases.\n",
    "\n",
    "* Some questions to ponder:\n",
    "  * Can you understand why the signal and background amplitudes are anticorrelated?\n",
    "  * Can you understand the difference in widths of the pdfs in the four cases?\n",
    "  * What are your conclusions for how to design the experiment given limited resources? \n",
    "  * In particular, given that you wanted to be able to distinguish between signal amplitude and background, would it then be better to have many counts in few bins, or the same total amount of counts spread over a wider interval? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: MCMC sampling of a Lorentzian pdf using the random walk Metropolis algorithm\n",
    "### (3 basic points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we have some function that tells us the (possibly unnormalized) probability of a given position in a one-dimensional space. Note, however, that a key feature of the approach that we will implement here is that it can be extended to many dimensions. \n",
    "\n",
    "We will assume a known, specific form of this univariate pdf, namely a Lorentzian (Cauchy) distribution, but it might just as well be some very complicated function that can only be evaluated numerically. All that is needed is some function that, for each position in the parameter space, returns a probability density.\n",
    "\n",
    "Let us start by studying the pdf that we will be sampling from using a random walk (using the Metropolis algorithm outlined below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Not really needed, but nicer plots\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules needed for this exercise\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw a number of random samples from the standard Cauchy\n",
    "r = cauchy.rvs(size=1000)\n",
    "plt.hist(r, density=True, histtype='stepfilled', alpha=0.2, \n",
    "         range=(-10,10),bins=21);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This histogram corresponds to a finite sample from the pdf of a standard Cauchy (Lorentzian)\n",
    "$$ \n",
    "p(x | \\alpha=0, \\beta=1) = \\frac{1}{\\pi(1+x^2)}, \n",
    "$$\n",
    "with mean $\\alpha=0$ and FWHM $2\\beta = 2$.\n",
    "\n",
    "- How does this pdf compare with a standard normal distribution $\\mathcal{N}(x;\\mu=0,\\sigma^2=1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, turn the posterior into a callable function. You should deliberately remove the normalization to make the point that sampling can be made for an unnormalized pdf. Note that we will work directly with the pdf here (not taking the log as in previous examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert correct code here\n",
    "#\n",
    "def posterior_function(x):\n",
    "    # This is NOT correct!\n",
    "    return np.nan\n",
    "\n",
    "def normalized_posterior_function(x):\n",
    "    # This is NOT correct!\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the sampling. The code for a MCMC sampler that uses the Metropolis algorithm is enclosed below. However, it misses a few critical steps and it is your task to add them at the correct places.\n",
    "\n",
    "1. At first, you have the starting parameter position (that can be randomly chosen), lets fix it to the input argument `start_position`:\n",
    "\n",
    "```python\n",
    "current_position = start_position\n",
    "```\n",
    "\n",
    "Then, you propose to move (jump) from that position somewhere else (that's the Markov part). You can be very dumb or very sophisticated about how you come up with that proposal. The Metropolis sampler is very dumb and just takes a sample from a proposal distribution (here we again choose a normal distribution) centered around your current position (i.e. `current_position`) with a certain standard deviation (`proposal_width`) that will determine how far you propose jumps \n",
    "\n",
    "2. Use `scipy.stats.norm` to create `proposed_position` with a step based on a draw from `current_position` with a width `proposal_width`.\n",
    "\n",
    "Next, you evaluate whether that's a good place to jump to or not. We quantify this by computing the probability of the proposed position in parameter space, i.e. evaluating the posterior pdf at the proposed position. Usually you would use log probabilities but we omit this here.\n",
    "\n",
    "3. Compute both `p_current` and `p_proposal`.\n",
    "\n",
    "Up until now, we essentially have a hill-climbing algorithm that would just propose movements into random directions and only accept a jump if the `proposed_position` has higher likelihood than `current_position`. Eventually we'll get to `x = 0` (or close to it) from where no more moves will be possible. However, we want to get a posterior pdf so we'll also have to sometimes accept moves into the other direction. The key trick is by dividing the two probabilities,\n",
    "\n",
    "```python\n",
    "p_accept = p_proposal / p_current\n",
    "```\n",
    "\n",
    "we get an acceptance probability. Note that the acceptance probability is obtained by dividing the posterior of proposed parameter setting by the posterior of the current parameter setting. This implies that the posterior pdf does not necessarily need to be nomalized, the normalization factor will anyway be canceled out. \n",
    "\n",
    "You can see that if `p_proposal` is larger, that probability will be `> 1` and we'll definitely accept. However, if `p_current` is larger, say twice as large, there'll be a 50% chance of moving there which we will decide by drawing a random number.\n",
    "\n",
    "4. Add the acceptance step by comparing `p_accept` to a random number (uniform [0,1]). The `current_position` should be updated if the `accept` variable is `True`.\n",
    "\n",
    "This simple procedure gives us samples from the posterior.\n",
    "\n",
    "The code below also calls a fancy visualization function `plot_proposal` if the optional keyword argument `plot=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (non-functional) sampler algorithm\n",
    "\n",
    "def sampler(posterior_func, no_of_samples=4, start_position=.5, \n",
    "            proposal_width=1., plot=False):\n",
    "    # starting parameter position\n",
    "    \n",
    "    samples = [current_position]\n",
    "    for i in range(no_of_samples):\n",
    "        # suggest new position\n",
    "\n",
    "        # Compute posteriors of current and proposed position       \n",
    "        \n",
    "        # Acceptance probability\n",
    "        # Note that this part will be modified in the Metropolis-Hastings algorithm \n",
    "        # for which we also consider asymmetric proposal distributions\n",
    "        p_accept = p_proposal / p_current\n",
    "        \n",
    "        # Accept proposal?\n",
    "        \n",
    "        # Visualization\n",
    "        if plot:\n",
    "            assert no_of_samples < 11, \"Too many samples for visualization\"\n",
    "            plot_proposal(posterior_func, current_position, p_current, \n",
    "                          proposed_position, p_proposal, accept, samples, i)\n",
    "        \n",
    "        # Possibly update position\n",
    "        \n",
    "        samples.append(current_position)\n",
    "        \n",
    "    return np.array(samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display\n",
    "def plot_proposal(posterior_func, current_position, p_current, \n",
    "                  proposed_position, p_proposal, accepted, trace, i):\n",
    "    from copy import copy\n",
    "    trace = copy(trace)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "    fig.suptitle('Iteration %i' % (i + 1))\n",
    "    x = np.linspace(-5, 5, 5000)\n",
    "    color = 'g' if accepted else 'r'\n",
    "        \n",
    "    # Plot posterior\n",
    "    ax1.plot(x, posterior_func(x))\n",
    "    ax1.plot([current_position] * 2, [0, p_current], marker='o', color='b')\n",
    "    ax1.plot([proposed_position] * 2, [0, p_proposal], marker='o', color=color)\n",
    "    ax1.annotate(\"\", xy=(proposed_position, 0.2), xytext=(current_position, 0.2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "    ax1.set(ylabel='Probability Density', \\\n",
    "            title='current: posterior(mu=%.2f) = %.2f\\nproposal: posterior(mu=%.2f) = %.2f' \\\n",
    "            % (current_position, p_current, proposed_position, p_proposal))\n",
    "    \n",
    "    if accepted:\n",
    "        trace.append(proposed_position)\n",
    "    else:\n",
    "        trace.append(current_position)\n",
    "        \n",
    "    # Posterior histogram\n",
    "    ax2.plot(x, normalized_posterior_function(x)) # properly normalized\n",
    "    sns.distplot(trace, kde=False, norm_hist=True, ax=ax2)\n",
    "    ax2.axvline(current_position, color='b', linestyle='--', \n",
    "                label='current position')\n",
    "    ax2.axvline(proposed_position, color=color, linestyle='--', \n",
    "                label='proposed position')\n",
    "    ax2.annotate(\"\", xy=(proposed_position, 0.2), xytext=(current_position, 0.2),\n",
    "                 arrowprops=dict(arrowstyle=\"->\", lw=2.))\n",
    "\n",
    "    \n",
    "    ax3.plot(trace)\n",
    "    ax3.set(xlabel='iteration', ylabel='position', title='trace')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the sampling, we'll create plots for some quantities that are computed. Each row below is a single iteration through our Metropolis sampler. These tests will also allow you to check whether your sampler works as expected.\n",
    "\n",
    "The first column displays our unnormalized posterior distribution. This is for visualization only, normally we would not be able to plot a nice curve to show the posterior. Here, we plug in our $x$ proposals. The vertical lines represent our current position in blue and our proposed position in either red or green (rejected or accepted, respectively). \n",
    "\n",
    "The 2nd column is our posterior distribution. Here we are displaying the normalized posterior as the blue curve compared to the normalized histogram of samples (green bars) and the move that was just proposed.\n",
    "\n",
    "The 3rd column is our trace (i.e. the posterior samples of visited positions that we're generating). Note that we store a sample at each iteration, irrespective of whether the propsal was accepted or rejected. In the latter situation, we keep the previous position and the line just stays constant.\n",
    "\n",
    "Note that we always accept moves to relatively more likely $x$ values (in terms of their posterior density), but only sometimes to relatively less likely $x$ values, as can be seen already in the first iteration, and later in iterations 6, 7, and 8 (the iteration number can be found at the top center of each row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "samples = sampler(posterior_function, no_of_samples=8, start_position=.5, proposal_width=1., plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the magic of MCMC is that you just have to do that for a long time, and the samples that are generated in this way come from the posterior distribution of your model. There is a rigorous mathematical proof that guarantees this which we won't go into detail here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final task:\n",
    "Draw a lot of samples from your sampler (say 100.000) and plot:\n",
    "* The trace (i.e. the sequence of draws of your single parameter x)\n",
    "* A normalized histogram of the samples compared to the true posterior pdf (normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
