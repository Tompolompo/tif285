TITLE: Learning from data: Linear Regression
AUTHOR: Christian Forss√©n {copyright, 2018-present|CC BY-NC} at Department of Physics, Chalmers University of Technology, Sweden
AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
DATE: today


!split
======= Linear regression =======

!split
===== Why Linear Regression (aka Ordinary Least Squares) =====

Fitting a continuous function with linear parameterization in terms of the parameters  $\bm{\theta}$.
* Often used for fitting a continuous function!
* Gives an excellent introduction to central Machine Learning features with _understandable pedagogical_ links to other methods like _Neural Networks_, _Support Vector Machines_ etc
* Analytical expression for the fitting parameters $\bm{\theta}$
* Analytical expressions for statistical propertiers like mean values, variances, confidence intervals and more
* Analytical relation with probabilistic interpretations 
* Easy to introduce basic concepts like bias-variance tradeoff, cross-validation, resampling and regularization techniques and many other ML topics
* Easy to code! And links well with classification problems and logistic regression and neural networks
* Allows for _easy_ hands-on understanding of gradient descent methods


!split
=== Regression analysis, overarching aims  ===
!bblock

Regression modeling deals with the description of  the sampling distribution of a given random variable $y$ and how it varies as function of another variable or a set of such variables $\bm{x} =[x_0, x_1,\dots, x_{n-1}]^T$. 
The first variable is called the _dependent_, the _outcome_ or the _response_ variable while the set of variables $\bm{x}$ is called the _independent_ variable, or the _predictor_ variable or the _explanatory_ variable. 
 
A regression model $M$ aims at finding a likelihood function $p(\bm{y}\vert \bm{x},M,\mathcal{D}_n)$, that is the conditional distribution for $\bm{y}$ given the independent variable $\bm{x}$ and a model $M$ that has been trained on a data set $\mathcal{D}_n$. The data set consists of: 
* $n$ cases $i = 0, 1, 2, \dots, n-1$ 
* Response variable $y_i$ with $i = 0, 1, 2, \dots, n-1$. These are sometimes referred to as target, dependent or outcome.
* For each case there will be $p$ so-called explanatory (independent or predictor) variables $\bm{x}_i=[x_{i0}, x_{i1}, \dots, x_{ip-1}]$ with $i = 0, 1, 2, \dots, n-1$ and explanatory variables running from $0$ to $p-1$. See below for more explicit examples. 
  
 The goal of the regression analysis is to extract/exploit a relationship between $\bm{y}$ and $\bm{x}$ or to infer causal dependencies, and to make fits, and predictions, and many other things.
!eblock

!split
!bblock
The $p$ explanatory variables for the $n$ cases in the data set are normally represented by a matrix $\mathbf{X}$.

The matrix $\mathbf{X}$ is called the *design
matrix*. In addition, each case is also represented by its *response variable* $\bm{y}$. The aim of
regression analysis is to explain $\bm{y}$ in terms of
$\bm{X}$ through a functional relationship like $y_i =
f(\mathbf{X}_{i},\ast )$.

It is common to assume a linear relationship
between $\bm{X}$ and $\bm{y}$. This assumption gives rise to
the *linear regression model* where $\bm{\theta} = [\theta_0, \ldots,
\theta_{p-1}]^{T}$ are the *regression parameters*. Linear regression gives us a set of analytical equations for the parameters $\theta_j$.

!eblock



!split
===  Example: Liquid-drop model for nuclear binding energies  ===
!bblock
In order to understand the relation among the predictors $p$, the set of data $\mathcal{D}_n$ and the target (outcome, output etc) $\bm{y}$,
consider the model we discussed for describing nuclear binding energies. 

There we assumed that we could parametrize the data using a polynomial approximation based on the liquid drop model.
Assuming 
!bt
\[
BE(A,N,Z) = a_0+a_1A+a_2A^{2/3}+a_3 Z^2 A^{-1/3}+a_4 (N-Z)^2 A^{-1},
\]
!et
we have five predictors, that is the intercept (constant term, aka bias), the $A$ dependent term, the $A^{2/3}$ term and the $Z^2 A^{-1/3}$ and $(N-Z)^2 A^{-1}$ terms. Although the predictors are somewhat complicated functions of $A,N,Z$, we note that the $p=5$ regression parameters $\theta = (a_0, a_1, a_2, a_3, a_4)$ enter linearly. Furthermore we have $n$ cases. It means that our design matrix is a 
$p\times n$ matrix $\bm{X}$.

!eblock


!split
===== Polynomial basis functions =====
!bblock
Let us study a case from linear algebra where we aim at fitting a set of data $\bm{y}=[y_0,y_1,\dots,y_{n-1}]$. We could think of these data as a result of an experiment or a complicated numerical experiment. These data are functions of a variable $x$ so that for the data set we have $\bm{x}=[x_0,x_1,\dots,x_{n-1}]$ and $y_i = y(x_i)$ with $i=0,1,2,\dots,n-1$. The variable $x_i$ could represent a physical quantity like time, temperature, position etc. We assume that $y(x)$ is a smooth function. 

Now, we don't know $y(x)$ but we want to use the data that we have to fit a function which can allow us to make predictions for values of $y$ which are not in the present set. The perhaps simplest approach is to assume we can parametrize our function in terms of a polynomial $f(x)$ of degree $p-1$. Since we realize that our polynomial model might not represent $y(x)$ perfectly we also add an error term
!bt
\[
y=y(x) \rightarrow y(x_i)=f({x}_i)+\epsilon_i=\sum_{j=0}^{p-1} \theta_j x_i^j+\epsilon_i,
\]
!et
where $\epsilon_i$ is the error in our approximation. 

!eblock


!split
!bblock
For every set of values $y_i,x_i$ we have thus the corresponding set of equations
!bt
\begin{align*}
y_0&=\theta_0+\theta_1x_0^1+\theta_2x_0^2+\dots+\theta_{p-1}x_0^{p-1}+\epsilon_0\\
y_1&=\theta_0+\theta_1x_1^1+\theta_2x_1^2+\dots+\theta_{p-1}x_1^{p-1}+\epsilon_1\\
y_2&=\theta_0+\theta_1x_2^1+\theta_2x_2^2+\dots+\theta_{p-1}x_2^{p-1}+\epsilon_2\\
\dots & \dots \\
y_{n-1}&=\theta_0+\theta_1x_{n-1}^1+\theta_2x_{n-1}^2+\dots+\theta_{p-1}x_{n-1}^{p-1}+\epsilon_{n-1}.\\
\end{align*}
!et
!eblock


!split
!bblock
Defining the vectors
!bt
\[
\bm{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,
\]
!et
and
!bt
\[
\bm{\theta} = [\theta_0,\theta_1, \theta_2,\dots, \theta_{p-1}]^T,
\]
!et
and
!bt
\[
\bm{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T,
\]
!et
and the design matrix
!bt
\[
\bm{X}=
\begin{bmatrix} 
1& x_{0}^1 &x_{0}^2& \dots & \dots &x_{0}^{p-1}\\
1& x_{1}^1 &x_{1}^2& \dots & \dots &x_{1}^{p-1}\\
1& x_{2}^1 &x_{2}^2& \dots & \dots &x_{2}^{p-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
1& x_{n-1}^1 &x_{n-1}^2& \dots & \dots &x_{n-1}^{p-1}\\
\end{bmatrix} 
\]
!et
we can rewrite our equations as
!bt
\[
\bm{y} = \bm{X}\bm{\theta}+\bm{\epsilon}.
\]
!et
The above design matrix is called a "Vandermonde matrix":"https://en.wikipedia.org/wiki/Vandermonde_matrix".
!eblock


!split
=== General basis functions  ===
!bblock

We are obviously not limited to the above polynomial expansions.  We
could replace the various powers of $x$ with elements of Fourier
series or instead of $x_i^j$ we could have $\cos{(j x_i)}$ or $\sin{(j
x_i)}$, or time series or other orthogonal functions.  For every set
of values $y_i,x_i$ we can then generalize the equations to

!bt
\begin{align*}
y_0&=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{p-1}x_{0p-1}+\epsilon_0\\
y_1&=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{p-1}x_{1p-1}+\epsilon_1\\
y_2&=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{p-1}x_{2p-1}+\epsilon_2\\
\dots & \dots \\
y_{i}&=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{p-1}x_{ip-1}+\epsilon_i\\
\dots & \dots \\
y_{n-1}&=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{p-1}x_{n-1,p-1}+\epsilon_{n-1}.\\
\end{align*}
!et

!eblock


!split
!bblock
We redefine in turn the matrix $\bm{X}$ as
!bt
\[
\bm{X}=
\begin{bmatrix} 
x_{00}& x_{01} &x_{02}& \dots & \dots &x_{0,p-1}\\
x_{10}& x_{11} &x_{12}& \dots & \dots &x_{1,p-1}\\
x_{20}& x_{21} &x_{22}& \dots & \dots &x_{2,p-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
x_{n-1,0}& x_{n-1,1} &x_{n-1,2}& \dots & \dots &x_{n-1,p-1}\\
\end{bmatrix} 
\]
!et
and without loss of generality we rewrite again our equations as
!bt
\[
\bm{y} = \bm{X}\bm{\theta}+\bm{\epsilon}.
\]
!et
The left-hand side of this equation is kwown. The error vector $\bm{\epsilon}$ and the parameter vector $\bm{\theta}$ are unknown quantities. How can we obtain the optimal set of $\theta_i$ values? 
!eblock


!split
!bblock
We have defined the matrix $\bm{X}$ via the equations
!bt
\begin{align*}
y_0&=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{p-1}x_{0p-1}+\epsilon_0\\
y_1&=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{p-1}x_{1p-1}+\epsilon_1\\
y_2&=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{p-1}x_{2p-1}+\epsilon_1\\
\dots & \dots \\
y_{i}&=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{p-1}x_{ip-1}+\epsilon_1\\
\dots & \dots \\
y_{n-1}&=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{p-1}x_{n-1,p-1}+\epsilon_{n-1}.\\
\end{align*}
!et

Note that the design matrix 
 $\bm{X}\in {\mathbb{R}}^{n\times p}$, with the predictors refering to the column numbers and the entries $n$ being the row elements.
!eblock



!split
!bblock
With the above we use the design matrix to define the approximation $\bm{\tilde{y}}$ via the unknown quantity $\bm{\theta}$ as
!bt
\[
\bm{\tilde{y}}= \bm{X}\bm{\theta},
\]
!et
and in order to find the optimal parameters $\theta_i$ instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values $y_i$ (which represent hopefully the exact values) and the parameterized values $\tilde{y}_i$, namely
!bt
\[
C(\bm{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\bm{y}-\bm{\tilde{y}}\right)^T\left(\bm{y}-\bm{\tilde{y}}\right)\right\},
\]
!et
or using the matrix $\bm{X}$ and in a more compact matrix-vector notation as
!bt
\[
C(\bm{\theta})=\frac{1}{n}\left\{\left(\bm{y}-\bm{X}\bm{\theta}\right)^T\left(\bm{y}-\bm{X}\bm{\theta}\right)\right\}.
\]
!et
This function is one possible way to define the so-called _cost function_.



It is also common to define
the cost function as

!bt
\[
C(\bm{\theta})=\frac{1}{2n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2,
\]
!et
since when taking the first derivative with respect to the unknown parameters $\theta$, the factor of $2$ cancels out. 
!eblock


!split
!bblock

The function 
!bt
\[
C(\bm{\theta})=\frac{1}{n}\left\{\left(\bm{y}-\bm{X}\bm{\theta}\right)^T\left(\bm{y}-\bm{X}\bm{\theta}\right)\right\},
\]
!et
can be linked to the variance of the quantity $y_i$ if we interpret the latter as the mean value. 
When linking (see the discussion below) with the maximum likelihood approach, we will indeed interpret $y_i$ as a mean value
!bt
\[
y_{i}=\langle y_i \rangle = \theta_0x_{i,0}+\theta_1x_{i,1}+\theta_2x_{i,2}+\dots+\theta_{n-1}x_{i,n-1}+\epsilon_i,
\]
!et

where $\langle y_i \rangle$ is the mean value. Keep in mind also that
till now we have treated $y_i$ as the exact value. Normally, the
response (dependent or outcome) variable $y_i$ the outcome of a
numerical experiment or another type of experiment and is thus only an
approximation to the true value. It is then always accompanied by an
error estimate, often limited to a statistical error estimate given by
the standard deviation discussed earlier. In the discussion here we
will treat $y_i$ as our exact value for the response variable.

In order to find the parameters $\theta_i$ we will then minimize the spread of $C(\bm{\theta})$, that is we are going to solve the problem
!bt
\[
{\displaystyle \min_{\bm{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\bm{y}-\bm{X}\bm{\theta}\right)^T\left(\bm{y}-\bm{X}\bm{\theta}\right)\right\}.
\]
!et
In practical terms it means we will require
!bt
\[
\frac{\partial C(\bm{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)^2\right]=0, 
\]
!et
which results in
!bt
\[
\frac{\partial C(\bm{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_{ij}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)\right]=0, 
\]
!et
or in a matrix-vector form as
!bt
\[
\frac{\partial C(\bm{\theta})}{\partial \bm{\theta}} = 0 = \bm{X}^T\left( \bm{y}-\bm{X}\bm{\theta}\right).  
\]
!et


!eblock


!split
!bblock
We can rewrite
!bt
\[
\frac{\partial C(\bm{\theta})}{\partial \bm{\theta}} = 0 = \bm{X}^T\left( \bm{y}-\bm{X}\bm{\theta}\right),  
\]
!et
as
!bt
\[
\bm{X}^T\bm{y} = \bm{X}^T\bm{X}\bm{\theta},  
\]
!et
and if the matrix $\bm{X}^T\bm{X}$ is invertible we have the solution
!bt
\[
\bm{\theta} =\left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T\bm{y}.
\]
!et

We note also that since our design matrix is defined as $\bm{X}\in
{\mathbb{R}}^{n\times p}$, the product $\bm{X}^T\bm{X} \in
{\mathbb{R}}^{p\times p}$.  In the liquid drop model example from the Intro lecture, we had $p=5$ ($p \ll n$) meaning that we end up with inverting a small
$5\times 5$ matrix. This is a rather common situation, in many cases we end up with low-dimensional
matrices to invert, which
allow for the usage of direct linear algebra methods such as _LU_ decomposition or _Singular Value Decomposition_ (SVD) for finding the inverse of the matrix
$\bm{X}^T\bm{X}$. 
!eblock

!bblock
_Small question_: What kind of problems can we expect when inverting the matrix  $\bm{X}^T\bm{X}$?  
!eblock

!split
===== Training scores =====

We can easily test our fit by computing various _training scores_. Several such measures are used in machine learning applications. First we have the _Mean-Squared Error_ (MSE)
!bt
\[
\mathrm{MSE}(\bm{\theta}) = \frac{1}{n} \sum_{i=1}^n \left( y_{\mathrm{data},i} - y_{\mathrm{model},i}(\bm{\theta}) \right)^2,
\]
!et
where we have $n$ training data and our model is a function of the parameter vector $\bm{\theta}$.

Furthermore, we have the _mean absolute error_ (MAE) defined as.
!bt
\[
\mathrm{MAE}(\bm{\theta}) = \frac{1}{n} \sum_{i=1}^n \left| y_{\mathrm{data},i} - y_{\mathrm{model},i}(\bm{\theta}) \right|,
\]
!et

And the $R2$ score, also known as *coefficient of determination* is
!bt
\[
\mathrm{R2}(\bm{\theta}) = 1 - \frac{\sum_{i=1}^n \left( y_{\mathrm{data},i} - y_{\mathrm{model},i}(\bm{\theta}) \right)^2}{\sum_{i=1}^n \left( y_{\mathrm{data},i} - \bar{y}_\mathrm{model}(\bm{\theta}) \right)^2},
\]
!et
where $\bar{y}_\mathrm{model}(\bm{\theta}) = \frac{1}{n} \sum_{i=1}^n y_{\mathrm{model},i} (\bm{\theta})$ is the mean of the model predictions.


!split
=== The $\chi^2$ function  ===
!bblock

Normally, the response (dependent or outcome) variable $y_i$ is the
outcome of a numerical experiment or another type of experiment and is
thus only an approximation to the true value. It is then always
accompanied by an error estimate, often limited to a statistical error
estimate given by the standard deviation discussed earlier. 

Introducing the standard deviation $\sigma_i$ for each measurement
$y_i$ (assuming uncorrelated errors), we define the $\chi^2$ function as

!bt
\[
\chi^2(\bm{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\frac{\left(y_i-\tilde{y}_i\right)^2}{\sigma_i^2}=\frac{1}{n}\left\{\left(\bm{y}-\bm{\tilde{y}}\right)^T \bm{\Sigma}^{-1}\left(\bm{y}-\bm{\tilde{y}}\right)\right\},
\]
!et
where the matrix $\bm{\Sigma}$ is a diagonal $n \times n$ matrix with $\sigma_i^2$ as matrix elements. 

!eblock

!split
!bblock

In order to find the parameters $\theta_i$ we will then minimize the spread of $\chi^2(\bm{\theta})$ by requiring
!bt
\[
\frac{\partial \chi^2(\bm{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)^2\right]=0, 
\]
!et
which results in
!bt
\[
\frac{\partial \chi^2(\bm{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}\frac{x_{ij}}{\sigma_i}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)\right]=0, 
\]
!et
or in a matrix-vector form as
!bt
\[
\frac{\partial \chi^2(\bm{\theta})}{\partial \bm{\theta}} = 0 = \bm{A}^T\left( \bm{b}-\bm{A}\bm{\theta}\right).  
\]
!et
where we have defined the matrix $\bm{A} =\bm{X} \bm{\Sigma}^{-1/2}$ with matrix elements $a_{ij} = x_{ij}/\sigma_i$ and the vector $\bm{b}$ with elements $b_i = y_i/\sigma_i$.   
!eblock

!split
!bblock

We can rewrite
!bt
\[
\frac{\partial \chi^2(\bm{\theta})}{\partial \bm{\theta}} = 0 = \bm{A}^T\left( \bm{b}-\bm{A}\bm{\theta}\right),  
\]
!et
as
!bt
\[
\bm{A}^T\bm{b} = \bm{A}^T\bm{A}\bm{\theta},  
\]
!et
and if the matrix $\bm{A}^T\bm{A}$ is invertible we have the solution
!bt
\[
\bm{\theta} =\left(\bm{A}^T\bm{A}\right)^{-1}\bm{A}^T\bm{b}.
\]
!et
!eblock

!split
!bblock

If we then introduce the matrix
!bt
\[
\bm{H} =  \left(\bm{A}^T\bm{A}\right)^{-1},
\]
!et
we have then the following expression for the parameters $\theta_j$ (the matrix elements of $\bm{H}$ are $h_{ij}$)
!bt
\[
\theta_j = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}\frac{y_i}{\sigma_i}\frac{x_{ik}}{\sigma_i} = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}b_ia_{ik}
\]
!et
We state without proof the expression for the uncertainty  in the parameters $\theta_j$ as (we leave this as an exercise)
!bt
\[
\sigma^2(\theta_j) = \sum_{i=0}^{n-1}\sigma_i^2\left( \frac{\partial \theta_j}{\partial y_i}\right)^2, 
\]
!et
resulting in 
!bt
\[
\sigma^2(\theta_j) = \left(\sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}a_{ik}\right)\left(\sum_{l=0}^{p-1}h_{jl}\sum_{m=0}^{n-1}a_{ml}\right) = h_{jj}!
\]
!et
!eblock